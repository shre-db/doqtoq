__module_name__ = "embedder"

from langchain_huggingface import HuggingFaceEmbeddings
from langchain.embeddings.base import Embeddings
from typing import Literal, Dict, Any, List
import os
from dotenv import load_dotenv

load_dotenv()

EmbeddingProvider = Literal["huggingface", "mistral"]

# Predefined model configurations for different providers
# Model Providers and Organizations:
# - sentence-transformers: all-MiniLM-L6-v2, all-mpnet-base-v2
# - BAAI (Beijing Academy of Artificial Intelligence): bge-small-en-v1.5, bge-base-en-v1.5, bge-large-en-v1.5
# - mistralai: mistral-embed
EMBEDDING_MODELS = {
    "huggingface": {
        # Sentence Transformers models
        "all-MiniLM-L6-v2": "sentence-transformers/all-MiniLM-L6-v2",
        "all-mpnet-base-v2": "sentence-transformers/all-mpnet-base-v2",
        # BAAI (Beijing Academy of Artificial Intelligence) models
        "bge-small-en-v1.5": "BAAI/bge-small-en-v1.5",
        "bge-base-en-v1.5": "BAAI/bge-base-en-v1.5",
        "bge-large-en-v1.5": "BAAI/bge-large-en-v1.5"
    },
    "mistral": {
        # Mistral AI models
        "mistral-embed": "mistral-embed"
    }
}

class MistralEmbeddings(Embeddings):
    """Custom Mistral embeddings wrapper for LangChain compatibility."""
    
    def __init__(self, model: str = "mistral-embed", api_key: str = None, **kwargs):
        """
        Initialize Mistral embeddings.
        
        Args:
            model: The Mistral model to use
            api_key: Mistral API key (if not provided, will use MISTRAL_API_KEY env var)
            **kwargs: Additional parameters
        """
        try:
            from mistralai import Mistral
        except ImportError:
            raise ImportError("mistralai package not found. Please install it with: pip install mistralai")
        
        self.model = model
        self.api_key = api_key or os.getenv("MISTRAL_API_KEY")
        
        if not self.api_key:
            raise ValueError("MISTRAL_API_KEY not found. Please set it in your .env file or pass it as api_key parameter.")
        
        self.client = Mistral(api_key=self.api_key)
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Embed a list of documents."""
        try:
            response = self.client.embeddings.create(
                model=self.model,
                inputs=texts
            )
            return [data.embedding for data in response.data]
        except Exception as e:
            raise RuntimeError(f"Failed to embed documents with Mistral: {str(e)}")
    
    def embed_query(self, text: str) -> List[float]:
        """Embed a single query text."""
        try:
            response = self.client.embeddings.create(
                model=self.model,
                inputs=[text]
            )
            return response.data[0].embedding
        except Exception as e:
            raise RuntimeError(f"Failed to embed query with Mistral: {str(e)}")

def get_embedding_model(
    provider: EmbeddingProvider = "huggingface",
    model_name: str = "all-MiniLM-L6-v2",
    **kwargs
):
    """
    Get an embedding model from various providers.
    
    Args:
        provider: The embedding provider to use
        model_name: The model name (short name from EMBEDDING_MODELS)
        **kwargs: Additional parameters for the embedding model
    
    Returns:
        Configured embedding model instance
    """
    if provider == "huggingface":
        return _get_huggingface_embeddings(model_name, **kwargs)
    elif provider == "mistral":
        return _get_mistral_embeddings(model_name, **kwargs)
    else:
        raise ValueError(f"Unsupported embedding provider: {provider}")

def _get_huggingface_embeddings(model_name: str, **kwargs):
    """Get HuggingFace BGE embeddings."""
    full_model_name = EMBEDDING_MODELS["huggingface"].get(model_name, model_name)
    
    default_kwargs = {
        "model_name": full_model_name,
        "model_kwargs": {"device": "cpu"},  # Can be overridden
        "encode_kwargs": {"normalize_embeddings": True}
    }
    default_kwargs.update(kwargs)
    
    return HuggingFaceEmbeddings(**default_kwargs)

def _get_mistral_embeddings(model_name: str, **kwargs):
    """Get Mistral embeddings."""
    full_model_name = EMBEDDING_MODELS["mistral"].get(model_name, model_name)
    
    default_kwargs = {
        "model": full_model_name
    }
    default_kwargs.update(kwargs)
    
    return MistralEmbeddings(**default_kwargs)

def list_available_models(provider: EmbeddingProvider = None) -> Dict[str, Dict[str, str]]:
    """
    List all available embedding models.
    
    Args:
        provider: Optional provider to filter by
    
    Returns:
        Dictionary of available models by provider
    """
    if provider:
        return {provider: EMBEDDING_MODELS.get(provider, {})}
    return EMBEDDING_MODELS

def get_model_info(provider: EmbeddingProvider, model_name: str) -> Dict[str, Any]:
    """
    Get information about a specific embedding model.
    
    Args:
        provider: The embedding provider
        model_name: The model name
    
    Returns:
        Dictionary with model information
    """
    models = EMBEDDING_MODELS.get(provider, {})
    if model_name not in models:
        return {"error": f"Model {model_name} not found for provider {provider}"}
    
    info = {
        "provider": provider,
        "model_name": model_name,
        "full_model_path": models[model_name],
        "requires_api_key": provider == "mistral",
    }
    
    # Add provider-specific information
    if provider == "huggingface":
        # Determine the actual model creator/organization
        if model_name.startswith("bge-"):
            info.update({
                "type": "BGE (Beijing Academy of Artificial Intelligence - BAAI)",
                "organization": "BAAI",
                "local": True,
                "gpu_recommended": "bge-large" in model_name,
                "description": "BGE (Beijing Academy of Artificial Intelligence) embedding model"
            })
        elif model_name in ["all-MiniLM-L6-v2", "all-mpnet-base-v2"]:
            info.update({
                "type": "Sentence Transformers",
                "organization": "sentence-transformers",
                "local": True,
                "gpu_recommended": "mpnet" in model_name,
                "description": "Sentence Transformers embedding model"
            })
        else:
            info.update({
                "type": "HuggingFace",
                "organization": "huggingface",
                "local": True,
                "gpu_recommended": False
            })
    elif provider == "mistral":
        info.update({
            "type": "Mistral AI",
            "organization": "mistralai",
            "local": False,
            "api_based": True,
            "description": "Mistral AI embedding model"
        })
    
    return info

def get_models_by_organization() -> Dict[str, Dict[str, List[str]]]:
    """
    Get embedding models organized by their actual organizations/providers.
    
    Returns:
        Dictionary organized by provider -> organization -> list of models
    """
    models_by_org = {
        "huggingface": {
            "sentence-transformers": [
                "all-MiniLM-L6-v2",
                "all-mpnet-base-v2"
            ],
            "BAAI": [
                "bge-small-en-v1.5",
                "bge-base-en-v1.5", 
                "bge-large-en-v1.5"
            ]
        },
        "mistral": {
            "mistralai": [
                "mistral-embed"
            ]
        }
    }
    return models_by_org

def list_models_with_providers() -> Dict[str, Any]:
    """
    List all available models with detailed provider information.
    
    Returns:
        Dictionary with detailed model and provider information
    """
    result = {
        "summary": {
            "total_models": 0,
            "providers": list(EMBEDDING_MODELS.keys()),
            "organizations": []
        },
        "models_by_provider": {},
        "models_by_organization": get_models_by_organization()
    }
    
    # Count total models and get organizations
    organizations = set()
    for provider, models in EMBEDDING_MODELS.items():
        result["models_by_provider"][provider] = {}
        result["summary"]["total_models"] += len(models)
        
        for model_name in models.keys():
            model_info = get_model_info(provider, model_name)
            result["models_by_provider"][provider][model_name] = model_info
            if "organization" in model_info:
                organizations.add(model_info["organization"])
    
    result["summary"]["organizations"] = list(organizations)
    return result

# Backward compatibility
def get_embedding_model_legacy(model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
    """Legacy function for backward compatibility."""
    return HuggingFaceEmbeddings(model_name=model_name)
